{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcadete/LLM_4_Biz_16/blob/main/tech16_HW6_aisuite_models_MC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MwGM6anD3BR"
      },
      "source": [
        "# Using any model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGt_2NPdEjiT",
        "outputId": "18630ebd-ce59-43a0-ba4c-fc3907ce07f5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aisuite[all] in /usr/local/lib/python3.11/dist-packages (0.1.10)\n",
            "Requirement already satisfied: anthropic<0.31.0,>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.30.1)\n",
            "Requirement already satisfied: cohere<6.0.0,>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (5.14.0)\n",
            "Requirement already satisfied: groq<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.9.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.27.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.21.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (1.10.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.3)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.0.20250306)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install 'aisuite[all]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjCPr5BGEtCu"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "anthrophic_key = userdata.get('claude_key')\n",
        "hf_key = userdata.get('hf_token')\n",
        "os.environ[\"OPENAI_API_KEY\"] = open_ai_key\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = anthrophic_key\n",
        "os.environ[\"HF_TOKEN\"] = hf_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCadIhxKE_6e",
        "outputId": "88c754a5-c8db-4305-e933-9da22c973e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating an AI chat application using Streamlit and Google Colab involves several steps. Below is a clear, step-by-step guide to help you build a simple chat application:\n",
            "\n",
            "### Step 1: Set Up Your Environment\n",
            "\n",
            "1. **Google Colab Setup:**\n",
            "   - Open Google Colab and create a new notebook.\n",
            "   - Ensure you have access to a GPU by navigating to `Runtime > Change runtime type` and selecting `GPU`.\n",
            "\n",
            "2. **Streamlit Setup:**\n",
            "   - Install Streamlit in your local environment or a virtual environment using the command:\n",
            "     ```bash\n",
            "     pip install streamlit\n",
            "     ```\n",
            "\n",
            "### Step 2: Develop the Chatbot Model in Google Colab\n",
            "\n",
            "1. **Import Required Libraries:**\n",
            "   - Use libraries like `transformers`, `torch`, and `numpy` for model development.\n",
            "     ```python\n",
            "     !pip install transformers\n",
            "     import torch\n",
            "     from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "     ```\n",
            "\n",
            "2. **Load a Pre-trained Model:**\n",
            "   - Use a pre-trained model like GPT-2 for the chatbot.\n",
            "     ```python\n",
            "     tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
            "     model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "     ```\n",
            "\n",
            "3. **Create a Function for Generating Responses:**\n",
            "   - Define a function that takes user input and generates a response using the model.\n",
            "     ```python\n",
            "     def generate_response(user_input):\n",
            "         inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
            "         outputs = model.generate(inputs, max_length=100, num_return_sequences=1)\n",
            "         response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
            "         return response\n",
            "     ```\n",
            "\n",
            "4. **Test the Model in Colab:**\n",
            "   - Ensure the function works by inputting test sentences and checking the responses.\n",
            "\n",
            "### Step 3: Develop the Streamlit Application\n",
            "\n",
            "1. **Create a New Python Script:**\n",
            "   - Create a new file, e.g., `app.py`, for the Streamlit application.\n",
            "\n",
            "2. **Import Streamlit and Your Model Functions:**\n",
            "   - Import Streamlit and any required functions from your Colab notebook (or save the model functions to a Python file and use them).\n",
            "     ```python\n",
            "     import streamlit as st\n",
            "     from my_model import generate_response  # Assuming you saved your function in my_model.py\n",
            "     ```\n",
            "\n",
            "3. **Build the Streamlit UI:**\n",
            "   - Use Streamlit components to build a simple user interface.\n",
            "     ```python\n",
            "     st.title(\"AI Chatbot\")\n",
            "     user_input = st.text_input(\"You: \", \"Hello, how are you?\")\n",
            "     if user_input:\n",
            "         response = generate_response(user_input)\n",
            "         st.write(\"Bot: \", response)\n",
            "     ```\n",
            "\n",
            "### Step 4: Run the Streamlit Application\n",
            "\n",
            "1. **Launch the Application:**\n",
            "   - Run the Streamlit app using the command:\n",
            "     ```bash\n",
            "     streamlit run app.py\n",
            "     ```\n",
            "\n",
            "2. **Interact with Your Chatbot:**\n",
            "   - Open the provided local URL in your browser to start interacting with the chatbot.\n",
            "\n",
            "### Step 5: Deploy the Application (Optional)\n",
            "\n",
            "- **Deploy to Streamlit Sharing or Similar Services:**\n",
            "  - Create a GitHub repository and push your code.\n",
            "  - Deploy the Streamlit app using Streamlit Community Cloud or another cloud service.\n",
            "\n",
            "By following these steps, you will have a functional AI chat application running locally. Adjust the model and UI as necessary to fit your specific needs and improve the chatbot experience.\n",
            "Certainly! Here's a clear step-by-step guide to creating an AI chat application using Streamlit and Google Colab:\n",
            "\n",
            "Step 1: Set up the environment\n",
            "1. Open Google Colab (colab.research.google.com).\n",
            "2. Create a new notebook or open an existing one.\n",
            "3. Install the required libraries by running the following commands in a code cell:\n",
            "   ```\n",
            "   !pip install streamlit\n",
            "   !pip install transformers\n",
            "   ```\n",
            "\n",
            "Step 2: Import necessary libraries\n",
            "1. In a new code cell, import the required libraries:\n",
            "   ```python\n",
            "   import streamlit as st\n",
            "   from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "   ```\n",
            "\n",
            "Step 3: Load the pre-trained model and tokenizer\n",
            "1. Choose a pre-trained language model (e.g., GPT-2) and load it along with its tokenizer:\n",
            "   ```python\n",
            "   model_name = \"gpt2\"\n",
            "   model = AutoModelForCausalLM.from_pretrained(model_name)\n",
            "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "   ```\n",
            "\n",
            "Step 4: Create the Streamlit app\n",
            "1. Define a function to generate responses from the model:\n",
            "   ```python\n",
            "   def generate_response(prompt):\n",
            "       input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
            "       output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
            "       response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
            "       return response\n",
            "   ```\n",
            "\n",
            "2. Create the Streamlit app:\n",
            "   ```python\n",
            "   def main():\n",
            "       st.title(\"AI Chat Application\")\n",
            "       \n",
            "       # Get user input\n",
            "       user_input = st.text_input(\"You:\", \"\")\n",
            "       \n",
            "       if user_input:\n",
            "           # Generate response\n",
            "           response = generate_response(user_input)\n",
            "           \n",
            "           # Display response\n",
            "           st.text_area(\"AI:\", value=response, height=200, max_chars=None)\n",
            "\n",
            "   if __name__ == \"__main__\":\n",
            "       main()\n",
            "   ```\n",
            "\n",
            "Step 5: Run the Streamlit app\n",
            "1. Add the following code at the end of the notebook to run the Streamlit app:\n",
            "   ```python\n",
            "   !streamlit run app.py & npx localtunnel --port 8501\n",
            "   ```\n",
            "\n",
            "2. Run the entire notebook.\n",
            "\n",
            "3. After running, you will see a URL generated by `npx localtunnel`. Click on the URL to open the Streamlit app in a new browser tab.\n",
            "\n",
            "Step 6: Interact with the AI chat application\n",
            "1. In the Streamlit app, you will see a text input field labeled \"You:\". Enter your message or question here.\n",
            "\n",
            "2. Press Enter or click outside the input field to generate a response from the AI.\n",
            "\n",
            "3. The AI-generated response will appear in the text area labeled \"AI:\".\n",
            "\n",
            "4. Continue the conversation by entering new messages in the \"You:\" input field.\n",
            "\n",
            "That's it! You now have a basic AI chat application built using Streamlit and Google Colab. You can further enhance the application by customizing the user interface, adding more features, or experimenting with different pre-trained models.\n",
            "\n",
            "Note: Keep in mind that the quality and coherence of the AI-generated responses depend on the chosen pre-trained model and the specific prompt provided.\n"
          ]
        }
      ],
      "source": [
        "import aisuite as ai\n",
        "client = ai.Client()\n",
        "\n",
        "models = [\"openai:gpt-4o\", \"anthropic:claude-3-opus-20240229\"]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are the most senior level ai staff engineer. respond in clear and concise english.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Provide a clear step by step guide to making an ai chat application using stremlit and google colab?\"},\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.75\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}