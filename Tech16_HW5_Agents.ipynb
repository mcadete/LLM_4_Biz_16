{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcadete/LLM_4_Biz_16/blob/main/Tech16_HW5_Agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"crewai[tools]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uvD1VYfesC06",
        "outputId": "c5f9e216-74a9-424e-f708-097ad866b15c",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting crewai[tools]\n",
            "  Downloading crewai-0.102.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting appdirs>=1.4.4 (from crewai[tools])\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting auth0-python>=4.7.1 (from crewai[tools])\n",
            "  Downloading auth0_python-4.8.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from crewai[tools]) (1.9.0)\n",
            "Collecting chromadb>=0.5.23 (from crewai[tools])\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crewai[tools]) (8.1.8)\n",
            "Collecting instructor>=1.3.3 (from crewai[tools])\n",
            "  Downloading instructor-1.7.2-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting json-repair>=0.25.2 (from crewai[tools])\n",
            "  Downloading json_repair-0.39.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting json5>=0.10.0 (from crewai[tools])\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting jsonref>=1.1.0 (from crewai[tools])\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting litellm==1.60.2 (from crewai[tools])\n",
            "  Downloading litellm-1.60.2-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: openai>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from crewai[tools]) (1.61.1)\n",
            "Requirement already satisfied: openpyxl>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from crewai[tools]) (3.1.5)\n",
            "Collecting opentelemetry-api>=1.22.0 (from crewai[tools])\n",
            "  Downloading opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.22.0 (from crewai[tools])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.22.0 (from crewai[tools])\n",
            "  Downloading opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting pdfplumber>=0.11.4 (from crewai[tools])\n",
            "  Downloading pdfplumber-0.11.5-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from crewai[tools]) (2.10.6)\n",
            "Collecting python-dotenv>=1.0.0 (from crewai[tools])\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting pyvis>=0.3.2 (from crewai[tools])\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.11/dist-packages (from crewai[tools]) (2024.11.6)\n",
            "Collecting tomli-w>=1.1.0 (from crewai[tools])\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tomli>=2.0.2 (from crewai[tools])\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting uv>=0.4.25 (from crewai[tools])\n",
            "  Downloading uv-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting crewai-tools>=0.36.0 (from crewai[tools])\n",
            "  Downloading crewai_tools-0.36.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai[tools]) (3.11.13)\n",
            "Collecting httpx<0.28.0,>=0.23.0 (from litellm==1.60.2->crewai[tools])\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai[tools]) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai[tools]) (3.1.5)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai[tools]) (4.23.0)\n",
            "Collecting tiktoken>=0.7.0 (from litellm==1.60.2->crewai[tools])\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai[tools]) (0.21.0)\n",
            "Requirement already satisfied: cryptography>=43.0.1 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai[tools]) (43.0.3)\n",
            "Requirement already satisfied: pyjwt>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai[tools]) (2.10.1)\n",
            "Requirement already satisfied: requests>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai[tools]) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai[tools]) (2.3.0)\n",
            "Collecting build>=1.0.3 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (1.26.4)\n",
            "Collecting posthog>=2.4.0 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading posthog-3.18.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (1.70.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (6.0.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (3.10.15)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai[tools]) (13.9.4)\n",
            "Collecting docker>=7.1.0 (from crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting embedchain>=0.1.114 (from crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading embedchain-0.1.127-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting lancedb>=0.5.4 (from crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading lancedb-0.20.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting pyright>=1.1.350 (from crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading pyright-1.1.396-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting pytube>=15.0.0 (from crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai[tools]) (0.16)\n",
            "Requirement already satisfied: jiter<0.9,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai[tools]) (0.8.2)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai[tools]) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai[tools]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai[tools]) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>=1.13.3->crewai[tools]) (1.3.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.5->crewai[tools]) (2.0.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.22.0->crewai[tools]) (1.2.18)\n",
            "Collecting importlib-metadata>=6.8.0 (from litellm==1.60.2->crewai[tools])\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai[tools]) (1.68.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai[tools])\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai[tools])\n",
            "  Downloading opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting protobuf<6.0,>=5.0 (from opentelemetry-proto==1.30.0->opentelemetry-exporter-otlp-proto-http>=1.22.0->crewai[tools])\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-sdk>=1.22.0->crewai[tools])\n",
            "  Downloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber>=0.11.4->crewai[tools])\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber>=0.11.4->crewai[tools]) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber>=0.11.4->crewai[tools])\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber>=0.11.4->crewai[tools]) (3.4.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.4.2->crewai[tools]) (0.7.0)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai[tools]) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai[tools]) (4.0.2)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai[tools]) (3.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai[tools]) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai[tools]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai[tools]) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai[tools]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai[tools]) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai[tools]) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai[tools]) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>=1.13.3->crewai[tools]) (3.10)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb>=0.5.23->crewai[tools]) (24.2)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=43.0.1->auth0-python>=4.7.1->crewai[tools]) (1.17.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.22.0->crewai[tools]) (1.17.2)\n",
            "Collecting alembic<2.0.0,>=1.13.1 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading alembic-1.14.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (4.13.3)\n",
            "Collecting chromadb>=0.5.23 (from crewai[tools])\n",
            "  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting cohere<6.0,>=5.3 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading cohere-5.14.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.26.1 in /usr/local/lib/python3.11/dist-packages (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (1.79.0)\n",
            "Collecting gptcache<0.2.0,>=0.1.43 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading gptcache-0.1.44-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (0.3.19)\n",
            "Collecting langchain-cohere<0.4.0,>=0.3.0 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading langchain_cohere-0.3.5-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting langchain-community<0.4.0,>=0.3.1 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading langchain_community-0.3.18-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-openai<0.3.0,>=0.2.1 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting mem0ai<0.2.0,>=0.1.54 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading mem0ai-0.1.60-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting pypdf<6.0.0,>=5.0.0 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading pypdf-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pysbd<0.4.0,>=0.3.4 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting schema<0.8.0,>=0.7.5 (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: sqlalchemy<3.0.0,>=2.0.27 in /usr/local/lib/python3.11/dist-packages (from embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2.0.38)\n",
            "Collecting tiktoken>=0.7.0 (from litellm==1.60.2->crewai[tools])\n",
            "  Downloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting tokenizers (from litellm==1.60.2->crewai[tools])\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.23.0->litellm==1.60.2->crewai[tools]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.23.0->litellm==1.60.2->crewai[tools]) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.23.0->litellm==1.60.2->crewai[tools]) (0.14.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm==1.60.2->crewai[tools]) (3.21.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools])\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.60.2->crewai[tools]) (3.0.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai[tools]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai[tools]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai[tools]) (0.23.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting deprecation (from lancedb>=0.5.4->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pylance~=0.23.2 (from lancedb>=0.5.4->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading pylance-0.23.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (7.2 kB)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai[tools]) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai[tools]) (1.13.1)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting nodeenv>=1.6.0 (from pyright>=1.1.350->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading nodeenv-1.9.1-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.5.23->crewai[tools]) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm==1.60.2->crewai[tools]) (0.28.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.5.23->crewai[tools]) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai[tools]) (14.2)\n",
            "Collecting Mako (from alembic<2.0.0,>=1.13.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading Mako-1.3.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.12.2->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=43.0.1->auth0-python>=4.7.1->crewai[tools]) (2.22)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting httpx-sse==0.4.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere<6.0,>=5.3->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading types_requests-2.32.0.20250301-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (4.9)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2.24.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (1.26.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (3.29.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (1.14.1)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2.0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.60.2->crewai[tools]) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.60.2->crewai[tools]) (2024.10.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (0.8.4)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (0.3.6)\n",
            "Collecting langchain-experimental<0.4.0,>=0.3.0 (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pandas>=1.4.3 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2.2.2)\n",
            "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (0.9.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.23->crewai[tools]) (0.1.2)\n",
            "Collecting pytz<2025.0,>=2024.1 (from mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting qdrant-client<2.0.0,>=1.9.1 (from mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading qdrant_client-1.13.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai[tools]) (0.2.13)\n",
            "Requirement already satisfied: pyarrow>=14 in /usr/local/lib/python3.11/dist-packages (from pylance~=0.23.2->lancedb>=0.5.4->crewai-tools>=0.36.0->crewai[tools]) (18.1.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3.0.0,>=2.0.27->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (3.1.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.23->crewai[tools])\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.23->crewai[tools]) (1.3.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (1.62.3)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=2.4.1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2.4.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (0.14.0)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.26.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (1.6.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (1.33)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.4.3->langchain-cohere<0.4.0,>=0.3.0->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (2025.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai[tools]) (0.6.1)\n",
            "Collecting grpcio-tools>=1.41.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (4.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.1->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools])\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client<2.0.0,>=1.9.1->mem0ai<0.2.0,>=0.1.54->embedchain>=0.1.114->crewai-tools>=0.36.0->crewai[tools]) (4.1.0)\n",
            "Downloading litellm-1.60.2-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading auth0_python-4.8.1-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crewai_tools-0.36.0-py3-none-any.whl (545 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m545.9/545.9 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crewai-0.102.0-py3-none-any.whl (240 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.2/240.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.7.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.39.1-py3-none-any.whl (20 kB)\n",
            "Downloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_http-1.30.0-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.5-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading uv-0.6.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.2/16.2 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading embedchain-0.1.127-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.4/211.4 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lancedb-0.20.0-cp39-abi3-manylinux_2_28_x86_64.whl (32.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.6/32.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.18.0-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyright-1.1.396-py3-none-any.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.14.1-py3-none-any.whl (233 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading cohere-5.14.0-py3-none-any.whl (253 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading gptcache-0.1.44-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_cohere-0.3.5-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.18-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mem0ai-0.1.60-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading nodeenv-1.9.1-py2.py3-none-any.whl (22 kB)\n",
            "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pylance-0.23.2-cp39-abi3-manylinux_2_28_x86_64.whl (35.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-5.3.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\n",
            "Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastavro-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading qdrant_client-1.13.2-py3-none-any.whl (306 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m306.6/306.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20250301-py3-none-any.whl (20 kB)\n",
            "Downloading Mako-1.3.9-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_tools-1.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53770 sha256=de49006c4aa6a7f555ac6861f7f0adb836ad4e60ee07cf4baca8bdebac63236a\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: schema, pytz, pypika, monotonic, durationpy, appdirs, uvloop, uvicorn, uv, types-requests, tomli-w, tomli, pytube, python-dotenv, pysbd, pyproject_hooks, pypdfium2, pypdf, pylance, protobuf, portalocker, overrides, opentelemetry-util-http, nodeenv, mypy-extensions, mmh3, marshmallow, Mako, jsonref, json5, json-repair, jedi, importlib-metadata, humanfriendly, httpx-sse, httptools, fastavro, deprecation, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, pyright, posthog, opentelemetry-proto, opentelemetry-api, httpx, grpcio-tools, gptcache, docker, coloredlogs, build, alembic, tokenizers, pyvis, pydantic-settings, pdfminer.six, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, langsmith, lancedb, kubernetes, fastapi, dataclasses-json, auth0-python, qdrant-client, pdfplumber, opentelemetry-sdk, opentelemetry-instrumentation, litellm, instructor, cohere, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, mem0ai, langchain-openai, opentelemetry-instrumentation-fastapi, langchain-community, chromadb, langchain-experimental, crewai, langchain-cohere, embedchain, crewai-tools\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.1\n",
            "    Uninstalling pytz-2025.1:\n",
            "      Successfully uninstalled pytz-2025.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.11\n",
            "    Uninstalling langsmith-0.3.11:\n",
            "      Successfully uninstalled langsmith-0.3.11\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.48.3 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.9 alembic-1.14.1 appdirs-1.4.4 asgiref-3.8.1 auth0-python-4.8.1 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 cohere-5.14.0 coloredlogs-15.0.1 crewai-0.102.0 crewai-tools-0.36.0 dataclasses-json-0.6.7 deprecation-2.1.0 docker-7.1.0 durationpy-0.9 embedchain-0.1.127 fastapi-0.115.11 fastavro-1.10.0 gptcache-0.1.44 grpcio-tools-1.70.0 httptools-0.6.4 httpx-0.27.2 httpx-sse-0.4.0 humanfriendly-10.0 importlib-metadata-8.5.0 instructor-1.7.2 jedi-0.19.2 json-repair-0.39.1 json5-0.10.0 jsonref-1.1.0 kubernetes-32.0.1 lancedb-0.20.0 langchain-cohere-0.3.5 langchain-community-0.3.18 langchain-experimental-0.3.4 langchain-openai-0.2.14 langsmith-0.1.147 litellm-1.60.2 marshmallow-3.26.1 mem0ai-0.1.60 mmh3-5.1.0 monotonic-1.6 mypy-extensions-1.0.0 nodeenv-1.9.1 onnxruntime-1.20.1 opentelemetry-api-1.30.0 opentelemetry-exporter-otlp-proto-common-1.30.0 opentelemetry-exporter-otlp-proto-grpc-1.30.0 opentelemetry-exporter-otlp-proto-http-1.30.0 opentelemetry-instrumentation-0.51b0 opentelemetry-instrumentation-asgi-0.51b0 opentelemetry-instrumentation-fastapi-0.51b0 opentelemetry-proto-1.30.0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 opentelemetry-util-http-0.51b0 overrides-7.7.0 pdfminer.six-20231228 pdfplumber-0.11.5 portalocker-2.10.1 posthog-3.18.0 protobuf-5.29.3 pydantic-settings-2.8.1 pylance-0.23.2 pypdf-5.3.1 pypdfium2-4.30.1 pypika-0.48.9 pyproject_hooks-1.2.0 pyright-1.1.396 pysbd-0.3.4 python-dotenv-1.0.1 pytube-15.0.0 pytz-2024.2 pyvis-0.3.2 qdrant-client-1.13.2 schema-0.7.7 starlette-0.46.0 tiktoken-0.7.0 tokenizers-0.20.3 tomli-2.2.1 tomli-w-1.2.0 types-requests-2.32.0.20250301 typing-inspect-0.9.0 uv-0.6.3 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata"
                ]
              },
              "id": "0e36913b5ceb4958881f1855fbbfbc3b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('open_ai_key')\n",
        "os.environ[\"SERPER_API_KEY\"] = userdata.get('SERPER_API_KEY')"
      ],
      "metadata": {
        "id": "WlLA99CoPMG6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple example"
      ],
      "metadata": {
        "id": "m0PQq1-wVZGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import SerperDevTool, WebsiteSearchTool\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Define the research topic\n",
        "topic = \"Implications for Nvidia on the Deepseek model\"\n",
        "\n",
        "# Create the Researcher agent\n",
        "researcher = Agent(\n",
        "    name=\"Researcher\",\n",
        "    role=\"Research Analyst\",\n",
        "    goal=\"Gather reliable information on a topic\",\n",
        "    backstory=\"An experienced researcher skilled in extracting key insights from online sources.\",\n",
        "    tools=[SerperDevTool(), WebsiteSearchTool()],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Create the Writer agent\n",
        "writer = Agent(\n",
        "    name=\"Writer\",\n",
        "    role=\"Content Writer\",\n",
        "    goal=\"Summarize research findings clearly\",\n",
        "    backstory=\"A skilled writer who crafts well-structured summaries from research data.\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Define tasks\n",
        "research_task = Task(\n",
        "    description=f\"Research '{topic}' and summarize key findings.\",\n",
        "    expected_output=f\"Bullet points summarizing '{topic}'.\",\n",
        "    agent=researcher\n",
        ")\n",
        "\n",
        "write_task = Task(\n",
        "    description=f\"Write a structured report on '{topic}' using research findings.\",\n",
        "    expected_output=f\"A markdown summary of '{topic}' with clear sections.\",\n",
        "    agent=writer,\n",
        "    context=[research_task]\n",
        ")\n",
        "\n",
        "# Assemble the crew and run tasks\n",
        "crew = Crew(\n",
        "    agents=[researcher, writer],\n",
        "    tasks=[research_task, write_task],\n",
        "    process=Process.sequential\n",
        ")\n",
        "\n",
        "result = crew.kickoff()\n",
        "\n",
        "# Display the final report\n",
        "display(Markdown(result.raw))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ORI-9x2CVchc",
        "outputId": "fbbdcf5e-6d67-49db-b599-77fd669460d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_config.py:295: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:502: UserWarning: <built-in function callable> is not a Python type (it may be an instance of an object), Pydantic will allow any object with no validation since we cannot even enforce that the input is an instance of the given type. To get rid of this error wrap the type with `pydantic.SkipValidation`.\n",
            "  warn(\n",
            "/usr/local/lib/python3.11/dist-packages/crewai_tools/tools/scrapegraph_scrape_tool/scrapegraph_scrape_tool.py:34: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"website_url\")\n",
            "/usr/local/lib/python3.11/dist-packages/crewai_tools/tools/selenium_scraping_tool/selenium_scraping_tool.py:26: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"website_url\")\n",
            "/usr/local/lib/python3.11/dist-packages/crewai_tools/tools/vision_tool/vision_tool.py:15: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  @validator(\"image_path_url\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Analyst\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mResearch 'Implications for Nvidia on the Deepseek model' and summarize key findings.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Analyst\u001b[00m\n",
            "\u001b[95m## Thought:\u001b[00m \u001b[92mI need to research the implications for Nvidia regarding the Deepseek model, gathering reliable and relevant information on this topic.\u001b[00m\n",
            "\u001b[95m## Using tool:\u001b[00m \u001b[92mSearch the internet with Serper\u001b[00m\n",
            "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
            "\"{\\\"search_query\\\": \\\"Implications for Nvidia on the Deepseek model\\\"}\"\u001b[00m\n",
            "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
            "{'searchParameters': {'q': 'Implications for Nvidia on the Deepseek model', 'type': 'search', 'num': 10, 'engine': 'google'}, 'organic': [{'title': \"How DeepSeek's AI Model Impacts Nvidia Stock - Nasdaq\", 'link': 'https://www.nasdaq.com/articles/how-deepseeks-ai-model-impacts-nvidia-stock', 'snippet': 'We believe this development could potentially have implications for Nvidia stock (NASDAQ:NVDA), the dominant player in the AI hardware space.', 'position': 1}, {'title': 'Nvidia, AI Chip Peers Tumble On DeepSeek Implications', 'link': 'https://www.investors.com/news/technology/nvidia-stock-ai-chipmakers-tumble-on-deepseek-news/', 'snippet': \"DeepSeek claims it used 2,048 Nvidia H800 chips, a downgraded version of Nvidia's H100 chips designed to comply with U.S. export restrictions. ...\", 'position': 2}, {'title': \"DeepSeek's AI Shockwave Hits Nvidia Hard, Wiping Out Billions\", 'link': 'https://www.forbes.com/sites/greatspeculations/2025/02/03/deepseeks-ai-shockwave-hits-nvidia-hard-wiping-out-billions/', 'snippet': \"DeepSeek's AI model, built at a fraction of the cost of leading U.S. models, signals the potential for a new price war in AI. Unlike ...\", 'position': 3}, {'title': \"Nvidia CEO shakes off DeepSeek effect, says 'new scaling law ...\", 'link': 'https://finance.yahoo.com/news/nvidia-ceo-shakes-off-deepseek-093000462.html', 'snippet': \"Nvidia CEO shakes off DeepSeek effect, says 'new scaling law ... GPU solutions following the launch of DeepSeek's V3 and R1 models.\", 'position': 4}, {'title': 'Taking Stock of the DeepSeek Shock - Stanford Cyber Policy Center', 'link': 'https://cyber.fsi.stanford.edu/publication/taking-stock-deepseek-shock', 'snippet': 'It did not take into account the investment it made to purchase thousands of varying models of Nvidia chips, and other infrastructure costs.', 'position': 5}, {'title': \"DeepSeek's Disruption: The Impact on Nvidia and the ... - VanEck\", 'link': 'https://www.vaneck.com/us/en/blogs/thematic-investing/deepseek-impact-on-nvidia/', 'snippet': \"DeepSeek's announcement—training a high-performance model on cheaper hardware—caused market jitters about future GPU demand. Previous ...\", 'position': 6}, {'title': \"Here's What I'm Doing With My Nvidia Shares After DeepSeek's ...\", 'link': 'https://www.fool.com/investing/2025/02/03/what-im-doing-with-my-nvidia-shares-after-deepseek/', 'snippet': \"The success of DeepSeek's R1 model had a jolting effect on Nvidia's shares. Yet, Nvidia GPUs will still be in high demand despite the ...\", 'position': 7}, {'title': 'The impact of competition and DeepSeek on Nvidia', 'link': 'https://simonwillison.net/2025/Jan/27/deepseek-nvidia/', 'snippet': 'With R1, DeepSeek essentially cracked one of the holy grails of AI: getting models to reason step-by-step without relying on massive supervised datasets.', 'position': 8}, {'title': \"Nvidia CEO Jensen Huang Says the DeepSeek Reaction ... - Barron's\", 'link': 'https://www.barrons.com/articles/deepseek-nvidia-ceo-jensen-huang-4ab1b0fa', 'snippet': \"DeepSeek's innovations will lead to greater demand for artificial-intelligence hardware, not less, says Nvidia CEO Jensen Huang.\", 'position': 9}], 'peopleAlsoAsk': [{'question': 'Is Nvidia good at AI?', 'snippet': 'NVIDIA has become a powerhouse in AI for a combination of reasons. Their GPUs are a standout, offering high parallel processing capabilities crucial for AI tasks. CUDA, their parallel computing platform, is widely used. Additionally, NVIDIA has been proactive in AI research, developing frameworks like CUDA-X AI.', 'title': 'Which companies (will) compete with NVIDIA for AI ? : r/stocks', 'link': 'https://www.reddit.com/r/stocks/comments/183y1p8/which_companies_will_compete_with_nvidia_for_ai/'}, {'question': 'How is Nvidia using AI in his business model?', 'snippet': 'By accelerating the entire AI workflow, projects reach production faster, with higher accuracy, efficiency, and infrastructure performance at a lower overall cost for various solutions and applications.', 'title': 'AI Solutions for Enterprises | NVIDIA', 'link': 'https://www.nvidia.com/en-au/solutions/ai/'}, {'question': 'What analysts say about Nvidia?', 'snippet': 'Of the 63 analysts covering the stock, 33 have a \"strong buy\" rating, as per LSEG data. The median price target stood at $175, implying that analysts expect a 33% increase from the stock\\'s Wednesday close.', 'title': \"Nvidia's optimistic forecast fails to convince Wall Street | Reuters\", 'link': 'https://www.reuters.com/technology/nvidia-shares-dip-forecast-fails-calm-overspending-fears-2025-02-27/'}], 'credits': 1}\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mResearch Analyst\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "- Market Reaction: The announcement of Deepseek's AI model caused a significant negative reaction in Nvidia's stock price, reflecting investor concerns about rising competition in the AI hardware sector. \n",
            "- Potential Price War: Deepseek's model is reported to have been developed at a much lower cost than existing models, indicating that it could trigger a new price war in AI technologies, which may affect Nvidia's pricing strategy and market share. \n",
            "- Impact on Hardware Demand: There are worries that cheaper, high-performing alternatives like Deepseek's model might reduce the demand for Nvidia's GPUs, which have been a critical revenue source for the company. \n",
            "- Response from Nvidia's CEO: Nvidia's CEO, Jensen Huang, has expressed confidence that innovations from competitors like Deepseek will ultimately drive greater demand for AI hardware rather than diminishing it, suggesting Nvidia's market position remains strong. \n",
            "- Investment in Technology: Deepseek's effective use of Nvidia chips in their model raises questions about the potential return on investment for Nvidia concerning their high-end chips, as cost-effective alternatives could attract customers looking for budget-friendly solutions. \n",
            "- Competitive Landscape: The introduction of Deepseek's AI model introduces a more competitive landscape for AI hardware, compelling Nvidia to innovate further and enhance the capabilities of their own products to maintain leadership in the industry.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Writer\u001b[00m\n",
            "\u001b[95m## Task:\u001b[00m \u001b[92mWrite a structured report on 'Implications for Nvidia on the Deepseek model' using research findings.\u001b[00m\n",
            "\n",
            "\n",
            "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mContent Writer\u001b[00m\n",
            "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
            "# Implications for Nvidia on the Deepseek Model\n",
            "\n",
            "## Introduction\n",
            "The recent launch of Deepseek's AI model has significant implications for Nvidia, highlighting both challenges and opportunities in the rapidly evolving AI hardware landscape. This report delves into the potential effects on Nvidia's market position, pricing strategy, and overall competitiveness in light of Deepseek's innovations.\n",
            "\n",
            "## Market Reaction\n",
            "Following the announcement of the Deepseek model, Nvidia experienced a notable decline in its stock price. This shift reflects investor apprehension regarding intensified competition in the AI hardware space. Investors are concerned that the emergence of Deepseek could undermine Nvidia's dominance, prompting a reassessment of future profitability.\n",
            "\n",
            "## Potential Price War\n",
            "Reports suggest that Deepseek's model was developed at a substantially lower cost than existing alternatives. This cost efficiency raises concerns about the possibility of a price war within the AI technology market. Should Deepseek aggressively price its offerings, Nvidia may be pressured to adjust its pricing strategy, which could lead to reduced profit margins and lost market share.\n",
            "\n",
            "## Impact on Hardware Demand\n",
            "As Deepseek introduces cost-effective and high-performance alternatives, Nvidia faces potential declines in the demand for its GPUs, which have historically been a core revenue driver. The shift towards lower-cost solutions indicates that budget-conscious consumers and businesses may gravitate toward Deepseek's offerings, challenging Nvidia's sales volumes.\n",
            "\n",
            "## Response from Nvidia's CEO\n",
            "In light of the competitive threat posed by Deepseek, Nvidia's CEO, Jensen Huang, remains optimistic. He posits that innovations from new entrants like Deepseek could stimulate broader demand for AI hardware, suggesting that increased competition may ultimately benefit Nvidia's market position. Huang's confidence implies that Nvidia may leverage its established brand and expertise to sustain its leadership amidst rising competition.\n",
            "\n",
            "## Investment in Technology\n",
            "The effective incorporation of Nvidia chips within Deepseek's AI model highlights a critical aspect of this competitive dynamic. While Deepseek's model can attract consumers with its budget-friendly solutions, it raises concerns about the return on investment for Nvidia's high-end chips. If Deepseek's model continues to capture market share, Nvidia may need to re-evaluate its product offerings to ensure they align with changing market demands and consumer preferences.\n",
            "\n",
            "## Competitive Landscape\n",
            "The introduction of Deepseek’s AI model has indeed reshaped the competitive landscape for AI hardware. With additional players entering the market, Nvidia is compelled to accelerate its innovation efforts. To maintain its industry leadership, Nvidia must enhance the capabilities of its products and develop strategies that differentiate its offerings from those of competitors.\n",
            "\n",
            "## Conclusion\n",
            "In summary, the implications of Deepseek's AI model for Nvidia are multifaceted, ranging from stock market reactions to pressures on pricing strategies and hardware demand. While challenges exist, Nvidia's proactive stance and commitment to innovation will be crucial in navigating this changing landscape. Continued vigilance and adaptation will enable Nvidia to thrive despite the emerging competition, reinforcing its position as a leader in AI hardware.\u001b[00m\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Implications for Nvidia on the Deepseek Model\n\n## Introduction\nThe recent launch of Deepseek's AI model has significant implications for Nvidia, highlighting both challenges and opportunities in the rapidly evolving AI hardware landscape. This report delves into the potential effects on Nvidia's market position, pricing strategy, and overall competitiveness in light of Deepseek's innovations.\n\n## Market Reaction\nFollowing the announcement of the Deepseek model, Nvidia experienced a notable decline in its stock price. This shift reflects investor apprehension regarding intensified competition in the AI hardware space. Investors are concerned that the emergence of Deepseek could undermine Nvidia's dominance, prompting a reassessment of future profitability.\n\n## Potential Price War\nReports suggest that Deepseek's model was developed at a substantially lower cost than existing alternatives. This cost efficiency raises concerns about the possibility of a price war within the AI technology market. Should Deepseek aggressively price its offerings, Nvidia may be pressured to adjust its pricing strategy, which could lead to reduced profit margins and lost market share.\n\n## Impact on Hardware Demand\nAs Deepseek introduces cost-effective and high-performance alternatives, Nvidia faces potential declines in the demand for its GPUs, which have historically been a core revenue driver. The shift towards lower-cost solutions indicates that budget-conscious consumers and businesses may gravitate toward Deepseek's offerings, challenging Nvidia's sales volumes.\n\n## Response from Nvidia's CEO\nIn light of the competitive threat posed by Deepseek, Nvidia's CEO, Jensen Huang, remains optimistic. He posits that innovations from new entrants like Deepseek could stimulate broader demand for AI hardware, suggesting that increased competition may ultimately benefit Nvidia's market position. Huang's confidence implies that Nvidia may leverage its established brand and expertise to sustain its leadership amidst rising competition.\n\n## Investment in Technology\nThe effective incorporation of Nvidia chips within Deepseek's AI model highlights a critical aspect of this competitive dynamic. While Deepseek's model can attract consumers with its budget-friendly solutions, it raises concerns about the return on investment for Nvidia's high-end chips. If Deepseek's model continues to capture market share, Nvidia may need to re-evaluate its product offerings to ensure they align with changing market demands and consumer preferences.\n\n## Competitive Landscape\nThe introduction of Deepseek’s AI model has indeed reshaped the competitive landscape for AI hardware. With additional players entering the market, Nvidia is compelled to accelerate its innovation efforts. To maintain its industry leadership, Nvidia must enhance the capabilities of its products and develop strategies that differentiate its offerings from those of competitors.\n\n## Conclusion\nIn summary, the implications of Deepseek's AI model for Nvidia are multifaceted, ranging from stock market reactions to pressures on pricing strategies and hardware demand. While challenges exist, Nvidia's proactive stance and commitment to innovation will be crucial in navigating this changing landscape. Continued vigilance and adaptation will enable Nvidia to thrive despite the emerging competition, reinforcing its position as a leader in AI hardware."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More detailed"
      ],
      "metadata": {
        "id": "GlBLos9yVbrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CrewAI components and tools\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import SerperDevTool, WebsiteSearchTool\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# --------------------------\n",
        "# Configuration for a generic research task\n",
        "# --------------------------\n",
        "# Set your research topic and task-specific descriptions.\n",
        "research_topic = \"What are the implications for Nvidia on the Deepseek model?\"\n",
        "research_description = (\n",
        "    f\"Research the topic '{research_topic}'. Provide a summary of key findings with supporting details.\"\n",
        ")\n",
        "research_expected_output = (\n",
        "    f\"A collection of bullet points detailing the major insights about '{research_topic}', including relevant sources.\"\n",
        ")\n",
        "\n",
        "writing_description = (\n",
        "    f\"Compose an analysis report summarizing the findings on '{research_topic}'. \"\n",
        "    \"The report should be structured, easy to read, and include clear headings and bullet points.\"\n",
        ")\n",
        "writing_expected_output = (\n",
        "    f\"A well-structured markdown summary of research findings on '{research_topic}', with clear headings and references.\"\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Set up the tools for the Researcher agent: a web search tool and a web content reader.\n",
        "# --------------------------\n",
        "search_tool = SerperDevTool()        # Enables web searching via the Serper API\n",
        "web_reader_tool = WebsiteSearchTool()  # Retrieves and reads content from web pages\n",
        "\n",
        "# --------------------------\n",
        "# Create the Researcher agent, whose role is to gather information from the web.\n",
        "# --------------------------\n",
        "researcher_agent = Agent(\n",
        "    name=\"Researcher\",\n",
        "    role=\"Research Analyst\",\n",
        "    goal=\"Gather reliable information on any given topic\",\n",
        "    backstory=\"An expert researcher adept at finding and curating information from diverse sources on the web.\",\n",
        "    tools=[search_tool, web_reader_tool],\n",
        "    verbose=True  # set to True to see internal processing details\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Create the Writer agent, whose role is to compile a summary report based on research.\n",
        "# --------------------------\n",
        "writer_agent = Agent(\n",
        "    name=\"Writer\",\n",
        "    role=\"Content Writer\",\n",
        "    goal=\"Summarize research findings into a clear report\",\n",
        "    backstory=\"A skilled writer who can explain complex information in simple terms.\",\n",
        "    tools=[],  # No special tools needed for this summarization task\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the research task for the Researcher agent.\n",
        "# --------------------------\n",
        "research_task = Task(\n",
        "    description=research_description,\n",
        "    expected_output=research_expected_output,\n",
        "    agent=researcher_agent\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the writing/summarization task for the Writer agent.\n",
        "# --------------------------\n",
        "write_task = Task(\n",
        "    description=writing_description,\n",
        "    expected_output=writing_expected_output,\n",
        "    agent=writer_agent,\n",
        "    context=[research_task]  # This task uses the output of the research_task as context\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Assemble the crew with both agents and tasks.\n",
        "# --------------------------\n",
        "research_crew = Crew(\n",
        "    agents=[researcher_agent, writer_agent],\n",
        "    tasks=[research_task, write_task],\n",
        "    process=Process.sequential,  # Runs the research task before the writing task\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Execute the crew's tasks\n",
        "result = research_crew.kickoff()\n",
        "\n",
        "# Render the final analysis report as markdown\n",
        "display(Markdown(result.raw))"
      ],
      "metadata": {
        "id": "DtYh4tiCT2cg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Include critic"
      ],
      "metadata": {
        "id": "FRFRPihi2Zhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CrewAI components and tools\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import SerperDevTool, WebsiteSearchTool\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# --------------------------\n",
        "# Configuration for a generic research task\n",
        "# --------------------------\n",
        "research_topic = \"What are the implications for Nvidia on the Deepseek model?\"\n",
        "research_description = (\n",
        "    f\"Research the topic '{research_topic}'. Provide a summary of key findings with supporting details.\"\n",
        ")\n",
        "research_expected_output = (\n",
        "    f\"A collection of bullet points detailing the major insights about '{research_topic}', including relevant sources.\"\n",
        ")\n",
        "\n",
        "writing_description = (\n",
        "    f\"Compose an analysis report summarizing the findings on '{research_topic}'. \"\n",
        "    \"The report should be structured, easy to read, and include clear headings and bullet points.\"\n",
        ")\n",
        "writing_expected_output = (\n",
        "    f\"A well-structured markdown summary of research findings on '{research_topic}', with clear headings and references.\"\n",
        ")\n",
        "\n",
        "critic_description = (\n",
        "    f\"Review the analysis report summarizing the findings on '{research_topic}'. \"\n",
        "    \"Provide detailed feedback on the clarity, coherence, and overall quality of the report, \"\n",
        "    \"including strengths, weaknesses, and suggestions for improvement.\"\n",
        ")\n",
        "critic_expected_output = (\n",
        "    \"A collection of bullet points or paragraphs providing constructive critique of the analysis report, \"\n",
        "    \"highlighting strengths, weaknesses, and potential improvements.\"\n",
        ")\n",
        "\n",
        "rewriter_description = (\n",
        "    f\"Rewrite the analysis report on '{research_topic}' by incorporating the feedback from the Critic. \"\n",
        "    \"Produce a refined version that addresses the criticisms and improves clarity and structure.\"\n",
        ")\n",
        "rewriter_expected_output = (\n",
        "    f\"A refined markdown analysis report on '{research_topic}', incorporating the critic's feedback, \"\n",
        "    \"with clear headings and detailed explanations.\"\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Set up the tools for the Researcher agent: a web search tool and a web content reader.\n",
        "# --------------------------\n",
        "search_tool = SerperDevTool()         # Enables web searching via the Serper API\n",
        "web_reader_tool = WebsiteSearchTool()   # Retrieves and reads content from web pages\n",
        "\n",
        "# --------------------------\n",
        "# Create the Researcher agent\n",
        "# --------------------------\n",
        "researcher_agent = Agent(\n",
        "    name=\"Researcher\",\n",
        "    role=\"Research Analyst\",\n",
        "    goal=\"Gather reliable information on any given topic\",\n",
        "    backstory=\"An expert researcher adept at finding and curating information from diverse sources on the web.\",\n",
        "    tools=[search_tool, web_reader_tool],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Create the Writer agent\n",
        "# --------------------------\n",
        "writer_agent = Agent(\n",
        "    name=\"Writer\",\n",
        "    role=\"Content Writer\",\n",
        "    goal=\"Summarize research findings into a clear report\",\n",
        "    backstory=\"A skilled writer who can explain complex information in simple terms.\",\n",
        "    tools=[],  # No special tools needed for this summarization task\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Create the Critic agent\n",
        "# --------------------------\n",
        "critic_agent = Agent(\n",
        "    name=\"Critic\",\n",
        "    role=\"Content Critic\",\n",
        "    goal=\"Review and provide constructive feedback on the analysis report\",\n",
        "    backstory=\"An expert in critically evaluating content to ensure quality, accuracy, and clarity.\",\n",
        "    tools=[],  # No special tools needed for this critique task\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Create the Rewriter agent\n",
        "# --------------------------\n",
        "rewriter_agent = Agent(\n",
        "    name=\"Rewriter\",\n",
        "    role=\"Content Editor\",\n",
        "    goal=\"Rewrite the analysis report by incorporating the critic's feedback\",\n",
        "    backstory=\"A skilled editor who refines content to enhance clarity, coherence, and overall quality.\",\n",
        "    tools=[],  # No special tools needed for this rewriting task\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the research task for the Researcher agent.\n",
        "# --------------------------\n",
        "research_task = Task(\n",
        "    description=research_description,\n",
        "    expected_output=research_expected_output,\n",
        "    agent=researcher_agent\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the writing/summarization task for the Writer agent.\n",
        "# --------------------------\n",
        "write_task = Task(\n",
        "    description=writing_description,\n",
        "    expected_output=writing_expected_output,\n",
        "    agent=writer_agent,\n",
        "    context=[research_task]  # This task uses the output of the research_task as context\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the critique task for the Critic agent.\n",
        "# --------------------------\n",
        "critic_task = Task(\n",
        "    description=critic_description,\n",
        "    expected_output=critic_expected_output,\n",
        "    agent=critic_agent,\n",
        "    context=[write_task]  # This task uses the output of the write_task as context\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the rewriting task for the Rewriter agent.\n",
        "# --------------------------\n",
        "rewriter_task = Task(\n",
        "    description=rewriter_description,\n",
        "    expected_output=rewriter_expected_output,\n",
        "    agent=rewriter_agent,\n",
        "    context=[write_task, critic_task]  # This task uses both the original report and the critique as context\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Assemble the crew with all four agents and tasks.\n",
        "# --------------------------\n",
        "research_crew = Crew(\n",
        "    agents=[researcher_agent, writer_agent, critic_agent, rewriter_agent],\n",
        "    tasks=[research_task, write_task, critic_task, rewriter_task],\n",
        "    process=Process.sequential,  # Executes tasks in sequence: research -> writing -> critique -> rewrite\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Execute the crew's tasks\n",
        "result = research_crew.kickoff()\n",
        "\n",
        "# Render the final refined report as markdown\n",
        "display(Markdown(result.raw))\n"
      ],
      "metadata": {
        "id": "yVFEgmC8zhp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hierarchical"
      ],
      "metadata": {
        "id": "ej0ZPVNDuxAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CrewAI components and tools\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import SerperDevTool, WebsiteSearchTool\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Import ChatOpenAI to serve as the manager LLM for the hierarchical process\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# --------------------------\n",
        "# Configuration for a generic research task\n",
        "# --------------------------\n",
        "research_topic = \"What are the implications for Nvidia on the Deepseek model?\"\n",
        "research_description = (\n",
        "    f\"Research the topic '{research_topic}'. Provide a summary of key findings with supporting details.\"\n",
        ")\n",
        "research_expected_output = (\n",
        "    f\"A collection of bullet points detailing the major insights about '{research_topic}', including relevant sources.\"\n",
        ")\n",
        "\n",
        "writing_description = (\n",
        "    f\"Compose an analysis report summarizing the findings on '{research_topic}'. \"\n",
        "    \"The report should be structured, easy to read, and include clear headings and bullet points.\"\n",
        ")\n",
        "writing_expected_output = (\n",
        "    f\"A well-structured markdown summary of research findings on '{research_topic}', with clear headings and references.\"\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Set up the tools for the Researcher agent: a web search tool and a web content reader.\n",
        "# --------------------------\n",
        "search_tool = SerperDevTool()        # Enables web searching via the Serper API\n",
        "web_reader_tool = WebsiteSearchTool()  # Retrieves and reads content from web pages\n",
        "\n",
        "# --------------------------\n",
        "# Create the Researcher agent, whose role is to gather information from the web.\n",
        "# --------------------------\n",
        "researcher_agent = Agent(\n",
        "    name=\"Researcher\",\n",
        "    role=\"Research Analyst\",\n",
        "    goal=\"Gather reliable information on any given topic\",\n",
        "    backstory=\"An expert researcher adept at finding and curating information from diverse sources on the web.\",\n",
        "    tools=[search_tool, web_reader_tool],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Create the Writer agent, whose role is to compile a summary report based on research.\n",
        "# --------------------------\n",
        "writer_agent = Agent(\n",
        "    name=\"Writer\",\n",
        "    role=\"Content Writer\",\n",
        "    goal=\"Summarize research findings into a clear report\",\n",
        "    backstory=\"A skilled writer who can explain complex information in simple terms.\",\n",
        "    tools=[],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the research task for the Researcher agent.\n",
        "# --------------------------\n",
        "research_task = Task(\n",
        "    description=research_description,\n",
        "    expected_output=research_expected_output,\n",
        "    agent=researcher_agent\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the writing/summarization task for the Writer agent.\n",
        "# --------------------------\n",
        "write_task = Task(\n",
        "    description=writing_description,\n",
        "    expected_output=writing_expected_output,\n",
        "    agent=writer_agent,\n",
        "    context=[research_task]  # This task uses the output of the research_task as context\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Set up the LLM-based manager for the hierarchical process.\n",
        "# --------------------------\n",
        "manager_llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "\n",
        "# --------------------------\n",
        "# Assemble the crew with both agents and tasks using a hierarchical process.\n",
        "# --------------------------\n",
        "research_crew = Crew(\n",
        "    agents=[researcher_agent, writer_agent],\n",
        "    tasks=[research_task, write_task],\n",
        "    process=Process.hierarchical,  # Use the hierarchical process\n",
        "    manager_llm=manager_llm,         # Provide the LLM-based manager required for hierarchical execution\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Execute the crew's tasks\n",
        "result = research_crew.kickoff()\n",
        "\n",
        "# Render the final analysis report as markdown\n",
        "display(Markdown(result.raw))\n"
      ],
      "metadata": {
        "id": "pWO-0X80uwbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Include critic"
      ],
      "metadata": {
        "id": "9k0B4mxe2QuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import CrewAI components and tools\n",
        "from crewai import Agent, Task, Crew, Process\n",
        "from crewai_tools import SerperDevTool, WebsiteSearchTool\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# Import ChatOpenAI to serve as the manager LLM for the hierarchical process\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# --------------------------\n",
        "# Configuration for a research task with critic feedback and revision\n",
        "# --------------------------\n",
        "research_topic = \"What are the implications for Nvidia on the Deepseek model?\"\n",
        "\n",
        "# Task descriptions and expected outputs\n",
        "research_description = (\n",
        "    f\"Research the topic '{research_topic}'. Provide a summary of key findings with supporting details.\"\n",
        ")\n",
        "research_expected_output = (\n",
        "    f\"A collection of bullet points detailing the major insights about '{research_topic}', including relevant sources.\"\n",
        ")\n",
        "\n",
        "writing_description = (\n",
        "    f\"Compose an initial analysis report summarizing the findings on '{research_topic}'. \"\n",
        "    \"The report should be structured, easy to read, and include clear headings and bullet points.\"\n",
        ")\n",
        "writing_expected_output = (\n",
        "    f\"A well-structured markdown summary of research findings on '{research_topic}', with clear headings and references.\"\n",
        ")\n",
        "\n",
        "critique_description = (\n",
        "    f\"Review the initial report on '{research_topic}' and provide detailed feedback focusing on structure, clarity, and insights. \"\n",
        "    \"Offer suggestions for improvement and highlight any gaps or areas needing clarification.\"\n",
        ")\n",
        "critique_expected_output = (\n",
        "    f\"A set of bullet points outlining the strengths, weaknesses, and actionable recommendations for the report on '{research_topic}'.\"\n",
        ")\n",
        "\n",
        "rewrite_description = (\n",
        "    f\"Rewrite the report on '{research_topic}' incorporating the feedback provided. \"\n",
        "    \"Ensure the revised report is improved in clarity, structure, and depth of analysis.\"\n",
        ")\n",
        "rewrite_expected_output = (\n",
        "    f\"An updated markdown summary of research findings on '{research_topic}', refined based on the critique feedback.\"\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Set up the tools for the Researcher agent: a web search tool and a web content reader.\n",
        "# --------------------------\n",
        "search_tool = SerperDevTool()        # Enables web searching via the Serper API\n",
        "web_reader_tool = WebsiteSearchTool()  # Retrieves and reads content from web pages\n",
        "\n",
        "# --------------------------\n",
        "# Create the Researcher agent, whose role is to gather information from the web.\n",
        "# --------------------------\n",
        "researcher_agent = Agent(\n",
        "    name=\"Researcher\",\n",
        "    role=\"Research Analyst\",\n",
        "    goal=\"Gather reliable information on any given topic\",\n",
        "    backstory=\"An expert researcher adept at finding and curating information from diverse sources on the web.\",\n",
        "    tools=[search_tool, web_reader_tool],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Create the Writer agent, whose role is to compile a summary report based on research.\n",
        "# --------------------------\n",
        "writer_agent = Agent(\n",
        "    name=\"Writer\",\n",
        "    role=\"Content Writer\",\n",
        "    goal=\"Summarize research findings into a clear report\",\n",
        "    backstory=\"A skilled writer who can explain complex information in simple terms.\",\n",
        "    tools=[],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Create the Critic agent, whose role is to provide detailed feedback on the report.\n",
        "# --------------------------\n",
        "critic_agent = Agent(\n",
        "    name=\"Critic\",\n",
        "    role=\"Content Critic\",\n",
        "    goal=\"Evaluate and provide constructive feedback on reports\",\n",
        "    backstory=\"An experienced critic with a keen eye for detail, focused on enhancing clarity, structure, and depth in reports.\",\n",
        "    tools=[],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the research task for the Researcher agent.\n",
        "# --------------------------\n",
        "research_task = Task(\n",
        "    description=research_description,\n",
        "    expected_output=research_expected_output,\n",
        "    agent=researcher_agent\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the initial writing task for the Writer agent.\n",
        "# --------------------------\n",
        "initial_write_task = Task(\n",
        "    description=writing_description,\n",
        "    expected_output=writing_expected_output,\n",
        "    agent=writer_agent,\n",
        "    context=[research_task]  # Uses the output of the research task as context\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the critique task for the Critic agent.\n",
        "# --------------------------\n",
        "critique_task = Task(\n",
        "    description=critique_description,\n",
        "    expected_output=critique_expected_output,\n",
        "    agent=critic_agent,\n",
        "    context=[initial_write_task]  # Uses the initial report as context\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Define the rewrite task for the Writer agent to update the report based on the critique.\n",
        "# --------------------------\n",
        "rewrite_task = Task(\n",
        "    description=rewrite_description,\n",
        "    expected_output=rewrite_expected_output,\n",
        "    agent=writer_agent,\n",
        "    context=[research_task, initial_write_task, critique_task]  # Uses both research and critique feedback as context\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Set up the LLM-based manager for the hierarchical process.\n",
        "# --------------------------\n",
        "manager_llm = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
        "\n",
        "# --------------------------\n",
        "# Assemble the crew with all agents and tasks using a hierarchical process.\n",
        "# --------------------------\n",
        "research_crew = Crew(\n",
        "    agents=[researcher_agent, writer_agent, critic_agent],\n",
        "    tasks=[research_task, initial_write_task, critique_task, rewrite_task],\n",
        "    process=Process.hierarchical,  # Use the hierarchical process\n",
        "    manager_llm=manager_llm,         # Provide the LLM-based manager required for hierarchical execution\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Execute the crew's tasks\n",
        "result = research_crew.kickoff()\n",
        "\n",
        "# Render the final updated analysis report as markdown\n",
        "display(Markdown(result.raw))\n"
      ],
      "metadata": {
        "id": "dQY_UjgxuxwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_P4lo9MyjGR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}