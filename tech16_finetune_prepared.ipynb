{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mcadete/LLM_4_Biz_16/blob/main/tech16_finetune_prepared.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MwGM6anD3BR"
      },
      "source": [
        "# Using any model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGt_2NPdEjiT",
        "outputId": "18630ebd-ce59-43a0-ba4c-fc3907ce07f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aisuite[all] in /usr/local/lib/python3.11/dist-packages (0.1.10)\n",
            "Requirement already satisfied: anthropic<0.31.0,>=0.30.1 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.30.1)\n",
            "Requirement already satisfied: cohere<6.0.0,>=5.12.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (5.14.0)\n",
            "Requirement already satisfied: groq<0.10.0,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.9.0)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (0.27.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.35.8 in /usr/local/lib/python3.11/dist-packages (from aisuite[all]) (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.21.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.11/dist-packages (from anthropic<0.31.0,>=0.30.1->aisuite[all]) (4.12.2)\n",
            "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (1.10.0)\n",
            "Requirement already satisfied: httpx-sse==0.4.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (0.4.0)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.27.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.3)\n",
            "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere<6.0.0,>=5.12.0->aisuite[all]) (2.32.0.20250306)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->aisuite[all]) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->aisuite[all]) (0.14.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.35.8->aisuite[all]) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere<6.0.0,>=5.12.0->aisuite[all]) (2.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic<0.31.0,>=0.30.1->aisuite[all]) (6.0.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install 'aisuite[all]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qjCPr5BGEtCu"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "anthrophic_key = userdata.get('claude_key')\n",
        "hf_key = userdata.get('hf_token')\n",
        "os.environ[\"OPENAI_API_KEY\"] = open_ai_key\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = anthrophic_key\n",
        "os.environ[\"HF_TOKEN\"] = hf_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCadIhxKE_6e",
        "outputId": "88c754a5-c8db-4305-e933-9da22c973e1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating an AI chat application using Streamlit and Google Colab involves several steps. Below is a clear, step-by-step guide to help you build a simple chat application:\n",
            "\n",
            "### Step 1: Set Up Your Environment\n",
            "\n",
            "1. **Google Colab Setup:**\n",
            "   - Open Google Colab and create a new notebook.\n",
            "   - Ensure you have access to a GPU by navigating to `Runtime > Change runtime type` and selecting `GPU`.\n",
            "\n",
            "2. **Streamlit Setup:**\n",
            "   - Install Streamlit in your local environment or a virtual environment using the command:\n",
            "     ```bash\n",
            "     pip install streamlit\n",
            "     ```\n",
            "\n",
            "### Step 2: Develop the Chatbot Model in Google Colab\n",
            "\n",
            "1. **Import Required Libraries:**\n",
            "   - Use libraries like `transformers`, `torch`, and `numpy` for model development.\n",
            "     ```python\n",
            "     !pip install transformers\n",
            "     import torch\n",
            "     from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "     ```\n",
            "\n",
            "2. **Load a Pre-trained Model:**\n",
            "   - Use a pre-trained model like GPT-2 for the chatbot.\n",
            "     ```python\n",
            "     tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
            "     model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
            "     ```\n",
            "\n",
            "3. **Create a Function for Generating Responses:**\n",
            "   - Define a function that takes user input and generates a response using the model.\n",
            "     ```python\n",
            "     def generate_response(user_input):\n",
            "         inputs = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
            "         outputs = model.generate(inputs, max_length=100, num_return_sequences=1)\n",
            "         response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
            "         return response\n",
            "     ```\n",
            "\n",
            "4. **Test the Model in Colab:**\n",
            "   - Ensure the function works by inputting test sentences and checking the responses.\n",
            "\n",
            "### Step 3: Develop the Streamlit Application\n",
            "\n",
            "1. **Create a New Python Script:**\n",
            "   - Create a new file, e.g., `app.py`, for the Streamlit application.\n",
            "\n",
            "2. **Import Streamlit and Your Model Functions:**\n",
            "   - Import Streamlit and any required functions from your Colab notebook (or save the model functions to a Python file and use them).\n",
            "     ```python\n",
            "     import streamlit as st\n",
            "     from my_model import generate_response  # Assuming you saved your function in my_model.py\n",
            "     ```\n",
            "\n",
            "3. **Build the Streamlit UI:**\n",
            "   - Use Streamlit components to build a simple user interface.\n",
            "     ```python\n",
            "     st.title(\"AI Chatbot\")\n",
            "     user_input = st.text_input(\"You: \", \"Hello, how are you?\")\n",
            "     if user_input:\n",
            "         response = generate_response(user_input)\n",
            "         st.write(\"Bot: \", response)\n",
            "     ```\n",
            "\n",
            "### Step 4: Run the Streamlit Application\n",
            "\n",
            "1. **Launch the Application:**\n",
            "   - Run the Streamlit app using the command:\n",
            "     ```bash\n",
            "     streamlit run app.py\n",
            "     ```\n",
            "\n",
            "2. **Interact with Your Chatbot:**\n",
            "   - Open the provided local URL in your browser to start interacting with the chatbot.\n",
            "\n",
            "### Step 5: Deploy the Application (Optional)\n",
            "\n",
            "- **Deploy to Streamlit Sharing or Similar Services:**\n",
            "  - Create a GitHub repository and push your code.\n",
            "  - Deploy the Streamlit app using Streamlit Community Cloud or another cloud service.\n",
            "\n",
            "By following these steps, you will have a functional AI chat application running locally. Adjust the model and UI as necessary to fit your specific needs and improve the chatbot experience.\n",
            "Certainly! Here's a clear step-by-step guide to creating an AI chat application using Streamlit and Google Colab:\n",
            "\n",
            "Step 1: Set up the environment\n",
            "1. Open Google Colab (colab.research.google.com).\n",
            "2. Create a new notebook or open an existing one.\n",
            "3. Install the required libraries by running the following commands in a code cell:\n",
            "   ```\n",
            "   !pip install streamlit\n",
            "   !pip install transformers\n",
            "   ```\n",
            "\n",
            "Step 2: Import necessary libraries\n",
            "1. In a new code cell, import the required libraries:\n",
            "   ```python\n",
            "   import streamlit as st\n",
            "   from transformers import AutoModelForCausalLM, AutoTokenizer\n",
            "   ```\n",
            "\n",
            "Step 3: Load the pre-trained model and tokenizer\n",
            "1. Choose a pre-trained language model (e.g., GPT-2) and load it along with its tokenizer:\n",
            "   ```python\n",
            "   model_name = \"gpt2\"\n",
            "   model = AutoModelForCausalLM.from_pretrained(model_name)\n",
            "   tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
            "   ```\n",
            "\n",
            "Step 4: Create the Streamlit app\n",
            "1. Define a function to generate responses from the model:\n",
            "   ```python\n",
            "   def generate_response(prompt):\n",
            "       input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
            "       output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
            "       response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
            "       return response\n",
            "   ```\n",
            "\n",
            "2. Create the Streamlit app:\n",
            "   ```python\n",
            "   def main():\n",
            "       st.title(\"AI Chat Application\")\n",
            "       \n",
            "       # Get user input\n",
            "       user_input = st.text_input(\"You:\", \"\")\n",
            "       \n",
            "       if user_input:\n",
            "           # Generate response\n",
            "           response = generate_response(user_input)\n",
            "           \n",
            "           # Display response\n",
            "           st.text_area(\"AI:\", value=response, height=200, max_chars=None)\n",
            "\n",
            "   if __name__ == \"__main__\":\n",
            "       main()\n",
            "   ```\n",
            "\n",
            "Step 5: Run the Streamlit app\n",
            "1. Add the following code at the end of the notebook to run the Streamlit app:\n",
            "   ```python\n",
            "   !streamlit run app.py & npx localtunnel --port 8501\n",
            "   ```\n",
            "\n",
            "2. Run the entire notebook.\n",
            "\n",
            "3. After running, you will see a URL generated by `npx localtunnel`. Click on the URL to open the Streamlit app in a new browser tab.\n",
            "\n",
            "Step 6: Interact with the AI chat application\n",
            "1. In the Streamlit app, you will see a text input field labeled \"You:\". Enter your message or question here.\n",
            "\n",
            "2. Press Enter or click outside the input field to generate a response from the AI.\n",
            "\n",
            "3. The AI-generated response will appear in the text area labeled \"AI:\".\n",
            "\n",
            "4. Continue the conversation by entering new messages in the \"You:\" input field.\n",
            "\n",
            "That's it! You now have a basic AI chat application built using Streamlit and Google Colab. You can further enhance the application by customizing the user interface, adding more features, or experimenting with different pre-trained models.\n",
            "\n",
            "Note: Keep in mind that the quality and coherence of the AI-generated responses depend on the chosen pre-trained model and the specific prompt provided.\n"
          ]
        }
      ],
      "source": [
        "import aisuite as ai\n",
        "client = ai.Client()\n",
        "\n",
        "models = [\"openai:gpt-4o\", \"anthropic:claude-3-opus-20240229\"]\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are the most senior level ai staff engineer. respond in clear and concise english.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Provide a clear step by step guide to making an ai chat application using stremlit and google colab?\"},\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0.75\n",
        "    )\n",
        "    print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu5xTIRiD0Oh"
      },
      "source": [
        "# Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_KOgX9_lydz",
        "outputId": "159b30f6-bfaa-4377-d128-41f3fc35bc53"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fsBYK7ZADwlQ",
        "outputId": "0a257926-eda7-4e2f-d3d4-01806b35ea92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model (this may take a while)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enabled LoRA for efficient fine-tuning with reduced rank.\n",
            "Starting fine-tuning...\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-5-2e09202b945c>\", line 105, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_hub/src/utils/pipeline_model.py\", line 177, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1065353216 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_76065]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2e09202b945c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# The training runs for 10 epochs, meaning the model sees the entire dataset 10 times.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# train_encodings = tokenizer(train_data, truncation=True, padding=True, return_tensors=\"tf\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fine-tuning complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras_hub/src/utils/pipeline_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, sample_weight, validation_data, validation_split, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m                 )\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         return super().fit(\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mkeras_symbolic_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeras_symbolic_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n\n  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n\n  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n\n  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n\n  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-5-2e09202b945c>\", line 105, in <cell line: 0>\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras_hub/src/utils/pipeline_model.py\", line 177, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 371, in fit\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 219, in function\n\n  File \"/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\", line 132, in multi_step_on_iterator\n\nOut of memory while trying to allocate 1065353216 bytes.\n\t [[{{node StatefulPartitionedCall}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_multi_step_on_iterator_76065]"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import keras_nlp  # A Keras-based library for natural language processing tasks.\n",
        "from tensorflow import keras\n",
        "from transformers import AutoTokenizer, TFAutoModel\n",
        "# Mixed Precision Training:\n",
        "# This enables the model to use both 16-bit and 32-bit floating-point types.\n",
        "# Using float16 for most operations reduces memory usage and speeds up computation,\n",
        "# while keeping some operations in float32 maintains stability.\n",
        "tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# ------------------------------\n",
        "# Load the Pre-trained Gemma Model\n",
        "# ------------------------------\n",
        "print(\"Loading model (this may take a while)...\")\n",
        "# This command loads a pre-trained language model named GemmaCausalLM from Hugging Face.\n",
        "# \"Causal\" means the model generates text in a sequential, left-to-right manner.\n",
        "\n",
        "# base_model = keras_nlp.models.Llama3CausalLM.from_preset(\n",
        "#   \"hf://deepseek-ai/DeepSeek-R1\"\n",
        "# )\n",
        "\n",
        "# base_model = TFAutoModel.from_pretrained(\"deepseek-ai/DeepSeek-R1\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1\")\n",
        "\n",
        "base_model = keras_nlp.models.GemmaCausalLM.from_preset(\n",
        "    \"hf://google/gemma-2-2b-it\"\n",
        ")\n",
        "\n",
        "# base_model = keras_nlp.models.Llama3CausalLM.from_preset(\n",
        "#     \"hf://meta-llama/Llama-3.2-1B\"\n",
        "# )\n",
        "\n",
        "\n",
        "# Display the structure of the model, including layers and number of parameters.\n",
        "base_model.summary()\n",
        "\n",
        "# ------------------------------\n",
        "# Enable LoRA Fine-Tuning\n",
        "# ------------------------------\n",
        "# LoRA (Low-Rank Adaptation) is a technique to efficiently fine-tune large models.\n",
        "# Instead of updating every parameter in the model (which can be millions or billions),\n",
        "# LoRA adds smaller matrices with a much lower rank (here, rank=2) to approximate the needed adjustments.\n",
        "# Think of it as fine-tuning by \"tweaking\" only a few parameters instead of re-writing a whole book.\n",
        "base_model.backbone.enable_lora(rank=2)\n",
        "print(\"Enabled LoRA for efficient fine-tuning with reduced rank.\")\n",
        "\n",
        "# ------------------------------\n",
        "# Prepare Training Data\n",
        "# ------------------------------\n",
        "# Here, we define a small dataset with pairs of symptoms and corresponding diseases.\n",
        "# Each string follows the format:\n",
        "# \"Symptom: <list of symptoms>.\\nDisease: <disease name>.\"\n",
        "# The \"\\n\" is a newline character that separates the symptoms from the disease.\n",
        "# train_data = [\n",
        "#     \"Symptom: persistent cough, fever, difficulty breathing.\\nDisease: Pneumonia.\",\n",
        "#     \"Symptom: severe headache, neck stiffness, photophobia.\\nDisease: Meningitis.\",\n",
        "#     \"Symptom: sudden weakness on one side, slurred speech.\\nDisease: Stroke.\",\n",
        "#     \"Symptom: increased thirst, frequent urination, unexplained weight loss.\\nDisease: Diabetes.\",\n",
        "#     \"Symptom: joint pain, prolonged morning stiffness, swelling in multiple joints.\\nDisease: Rheumatoid Arthritis.\"\n",
        "# ]\n",
        "\n",
        "# train_data = [\n",
        "#     \"Line Item: Starbucks, $5.67, 2025-02-28, Coffee Shop.\\nLabel: Not Fraud.\",\n",
        "#     \"Line Item: Unknown Merchant, $1200.00, 2025-02-27, Electronics.\\nLabel: Fraud.\",\n",
        "#     \"Line Item: Walmart Supercenter, $45.32, 2025-02-26, Groceries.\\nLabel: Not Fraud.\",\n",
        "#     \"Line Item: Luxury Boutique, $2200.00, 2025-02-28, Designer Clothing.\\nLabel: Fraud.\",\n",
        "#     \"Line Item: Uber, $18.75, 2025-02-27, Ride Share.\\nLabel: Not Fraud.\"\n",
        "# ]\n",
        "\n",
        "train_data = [\n",
        "    \"Quote: May the Force be with you. \\n: Movie: Star Wars, Character: Obi-Wan Kenobi, Release Date: 1977.\",\n",
        "    \"Quote: I'll be back.\\n Movie: The Terminator, Character: Terminator, Release Date: 1984.\",\n",
        "    \"Quote: I'm going to make him an offer he can't refuse.\\n: Movie: The Godfather, Character: Vito Corleone, Release Date: 1972.\",\n",
        "    \"Quote: Here's looking at you, kid.\\n Movie: Casablanca, Character: Rick Blaine, Release Date: 1942.\",\n",
        "    \"Quote: You talking to me?\\n Movie: Taxi Driver, Character: Travis Bickle, Release Date: 1976.\"\n",
        "]\n",
        "\n",
        "# ------------------------------\n",
        "# Compile the Model\n",
        "# ------------------------------\n",
        "# Before training, the model is compiled by specifying:\n",
        "# - A loss function: Measures how well the model's predictions match the actual labels.\n",
        "# - An optimizer: Determines how the model's weights are updated during training.\n",
        "# - Metrics: Additional measurements to judge performance (here, accuracy).\n",
        "base_model.compile(\n",
        "    # SparseCategoricalCrossentropy is used when you have multiple classes and your labels are integers.\n",
        "    # \"from_logits=True\" indicates that the model's outputs are raw values (logits), not probabilities.\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    # Adam optimizer is chosen for its ability to adjust learning rates during training.\n",
        "    # It combines ideas from momentum and adaptive learning rate techniques.\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=5e-5),\n",
        "    # SparseCategoricalAccuracy computes the percentage of correct predictions.\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()]\n",
        ")\n",
        "\n",
        "# ------------------------------\n",
        "# Fine-Tune the Model\n",
        "# ------------------------------\n",
        "print(\"Starting fine-tuning...\")\n",
        "# The model is fine-tuned (trained) on the provided training data.\n",
        "# Fine-tuning adjusts the model's weights to specialize in the new task (mapping symptoms to diseases).\n",
        "# A batch size of 1 is used, meaning one training sample is processed at a time.\n",
        "# The training runs for 10 epochs, meaning the model sees the entire dataset 10 times.\n",
        "# train_encodings = tokenizer(train_data, truncation=True, padding=True, return_tensors=\"tf\")\n",
        "base_model.fit(train_data, batch_size=1, epochs=2)\n",
        "print(\"Fine-tuning complete.\")\n",
        "\n",
        "# ------------------------------\n",
        "# Save the Fine-Tuned Model\n",
        "# ------------------------------\n",
        "# After training, the model is saved in the recommended .keras format.\n",
        "# This allows you to reuse the model later without retraining.\n",
        "base_model.save(\"fine_tuned_model.keras\")\n",
        "print(\"Model saved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMzsft1WD_rd"
      },
      "outputs": [],
      "source": [
        "# ------------------------------\n",
        "# Reload the Model for Inference\n",
        "# ------------------------------\n",
        "# The saved model is reloaded for performing inference (generating predictions).\n",
        "\n",
        "reloaded_model = keras.models.load_model(\"fine_tuned_model.keras\")\n",
        "print(\"Model reloaded for inference.\")\n",
        "\n",
        "# ------------------------------\n",
        "# Set Up a Sampler for Text Generation\n",
        "# ------------------------------\n",
        "# When generating text, a sampler helps decide the next token (word or subword).\n",
        "# GreedySampler always selects the token with the highest probability at each step.\n",
        "sampler = keras_nlp.samplers.GreedySampler()\n",
        "# The sampler is integrated into the model for use during inference.\n",
        "reloaded_model.compile(sampler=sampler)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I2qcD7okBBX"
      },
      "outputs": [],
      "source": [
        "# Generate an answer for a given healthcare-related symptom prompt\n",
        "# prompt = \"Symptom: sudden weakness on one side, slurred speech.\\nDisease:\"\n",
        "# prompt = \"Line Item: random merchant, $543.67, 2025-02-31, Retail.\\nLabel:\"\n",
        "prompt = \"Quote: Greed is good.\\n Movie:\"\n",
        "result = reloaded_model.generate(prompt, max_length=50)\n",
        "print(\"Generated Response:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2dW7uD_N8dm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}